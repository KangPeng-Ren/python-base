{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T04:17:17.717473100Z",
     "start_time": "2025-12-11T04:17:12.775428100Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:17:41.446342800Z",
     "start_time": "2025-12-11T04:17:41.380874800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义数据\n",
    "x = torch.tensor(10.0)\n",
    "y = torch.tensor(3.0)\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "9439a2b7472e1d1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:19:48.190828300Z",
     "start_time": "2025-12-11T04:19:48.151673800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化参数\n",
    "w = torch.rand(1, 1, requires_grad=True)\n",
    "b = torch.rand(1, 1, requires_grad=True)"
   ],
   "id": "3dd82fa0f6fb859c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:20:09.519909200Z",
     "start_time": "2025-12-11T04:20:09.453041400Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3294]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "# 前向传播得到输出值\n",
    "z = w * x + b\n",
    "print(z)"
   ],
   "id": "72687872fe099f92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:25:08.690758400Z",
     "start_time": "2025-12-11T04:25:08.661690900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 叶子节点会一直保存梯度，非叶子节点不会存储\n",
    "print(x.is_leaf)\n",
    "print(w.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(y.is_leaf)\n",
    "print(z.is_leaf)"
   ],
   "id": "24c37d462b7da44c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:29:00.615003800Z",
     "start_time": "2025-12-11T04:29:00.590450800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置损失函数\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_value = loss(z, y)\n",
    "print(loss_value)\n",
    "print(loss_value.is_leaf)"
   ],
   "id": "85540f9e35184672",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7674, grad_fn=<MseLossBackward0>)\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:31:06.356545300Z",
     "start_time": "2025-12-11T04:31:06.299771600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 反向传播\n",
    "loss_value.backward()"
   ],
   "id": "9cc10a55fae5a8d2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T04:31:57.126746600Z",
     "start_time": "2025-12-11T04:31:57.074895900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看梯度\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "2dbf4f2867fcab3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26.5888]])\n",
      "tensor([[2.6589]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "自动微分的关键就是记录节点的数据与运算。数据记录在张量的`data`属性中，计算记录在张量的`grad_fn`属性中。\n",
    "\n",
    "\n",
    "计算图根据搭建方式可分为静态图和动态图，`PyTorch`是动态图机制，在计算的过程中逐步搭建计算图，同时对每个`Tensor`都存储`grad_fn`供自动微分使用。\n",
    "\n",
    "\n",
    "若设置张量参数`requires_grad=True`，则`PyTorch`会追踪素有基于该张量的操作，并在反向传播时计算其梯度。以来于叶子节点的节点，`requires_grad`默认为`True`。当计算到根节点后，在根节点调用`backward()`方法即可反向传播计算计算图中所有节点的梯度。\n",
    "\n",
    "\n",
    "非叶子节点的梯度在反向传播之后会被释放掉（除非设置参数`retain_grad=True`）。而叶子节点的梯度在反向传播之后会保留（累积）。通常需要`optimizer.zero_grad()`清零参数的梯度。\n",
    "\n",
    "\n",
    "有时我们希望将某些计算移动到计算图之外，可以使用`Tensor.detach()`返回一个新的变量，该标量与原变量具有相同的值，但丢失计算图中如何计算原变量的信息。换句话说，梯度不会在该变量处继续乡下传播。"
   ],
   "id": "518f1ff0962df102"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3fb776544a87222d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
