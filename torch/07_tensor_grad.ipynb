{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:30.926934600Z",
     "start_time": "2025-12-22T04:36:28.662731300Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 自动微分模块",
   "id": "2510c70e58d23ab6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "PyTorch中具有一个内置的微分引擎`torch.autograd`以支持图的梯度自动计算",
   "id": "1d33d9766701246a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "考虑最简单的单层神经网络，具有输入x、参数w、偏置b以及损失函数（w、b为参数，CE：损失函数）\n",
    "| `x`  | -> | *  | -> | +  | -> | `z`  | -> | CE | -> | `loss` |\n",
    "| :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :--- |\n",
    "|    |    | `w`  |    | `b`  |    |    |    | `y`  |    |      |\n",
    "\n",
    "Tensor类中定义了梯度相关的属性和方法，首先，值记录在属性`data`中，通过布尔类型属性`requires_grad`来判断该张量是否需要梯度计算，计算出的梯度存放在`grad`属性中，计算梯度时需要知道计算图中发生了哪些操作，计算的函数记录在属性`grad_fn`中\n",
    "\n",
    "如果一个张量是通过另外张量计算得到的，那么若计算部分中的任一张量的`requires_grad`属性是开启的，那么此张量的`requires_grad`属性也是开启的\n",
    "\n",
    "**叶子节点：**`x`、`w`、`b`、`y`，并非由计算得出的，而是计算的起点，可以通过`is_leaf()`方法判断是否为叶子节点，反向传播计算梯度后，叶子节点中的`grad`属性会一直保存，非叶子节点不会自动保存，除非设置`retain_grad`属性\n",
    "\n",
    "**根节点：**`loss`"
   ],
   "id": "16c233c0145d0762"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.015901400Z",
     "start_time": "2025-12-22T04:36:30.927936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义数据\n",
    "x = torch.tensor(10.0)\n",
    "y = torch.tensor([[3.0]])\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "71beb938d3f96741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([[3.]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.071113600Z",
     "start_time": "2025-12-22T04:36:31.016898700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义参数，权重和偏置\n",
    "w = torch.rand(1, 1, requires_grad=True)  # 1 x 1的矩阵，需要计算梯度，开启requires_grad属性\n",
    "b = torch.rand(1, 1, requires_grad=True)  # 1 x 1的矩阵，需要计算梯度，开启requires_grad属性\n",
    "print(w)\n",
    "print(b)"
   ],
   "id": "f88cb26eb105c565",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8231]], requires_grad=True)\n",
      "tensor([[0.8497]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.127332900Z",
     "start_time": "2025-12-22T04:36:31.071782300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 前向传播，得到输出值\n",
    "z = w * x + b  # z为某些张量得到的一个新的张量\n",
    "print(z)"
   ],
   "id": "12fe555caa006f7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.0809]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.204352500Z",
     "start_time": "2025-12-22T04:36:31.129334100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x.is_leaf)\n",
    "print(w.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(y.is_leaf)\n",
    "print(z.is_leaf)"
   ],
   "id": "576c8c9d82c7ee05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.269569400Z",
     "start_time": "2025-12-22T04:36:31.205358700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置损失函数，均方误差\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_value = loss(z, y)  # 通过真实值和预测值计算误差\n",
    "print(loss_value)\n",
    "print(loss_value.is_leaf)"
   ],
   "id": "5a959ecc51fcd085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.9775, grad_fn=<MseLossBackward0>)\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.315112500Z",
     "start_time": "2025-12-22T04:36:31.269569400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 反向传播\n",
    "loss_value.backward()"
   ],
   "id": "40e891b88ec8fd90",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.375706900Z",
     "start_time": "2025-12-22T04:36:31.323116800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看梯度\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "2259fa1eee28c92b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[121.6183]])\n",
      "tensor([[12.1618]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "自动微分的关键就是记录节点的数据与运算。数据记录在张量的`data`属性中，计算记录在张量的`grad_fn`属性中。\n",
    "\n",
    "\n",
    "计算图根据搭建方式可分为静态图和动态图，`PyTorch`是动态图机制，在计算的过程中逐步搭建计算图，同时对每个`Tensor`都存储`grad_fn`供自动微分使用。\n",
    "\n",
    "\n",
    "若设置张量参数`requires_grad=True`，则`PyTorch`会追踪素有基于该张量的操作，并在反向传播时计算其梯度。以来于叶子节点的节点，`requires_grad`默认为`True`。当计算到根节点后，在根节点调用`backward()`方法即可反向传播计算计算图中所有节点的梯度。\n",
    "\n",
    "\n",
    "非叶子节点的梯度在反向传播之后会被释放掉（除非设置参数`retain_grad=True`）。而叶子节点的梯度在反向传播之后会保留（累积）。通常需要`optimizer.zero_grad()`清零参数的梯度。\n",
    "\n",
    "\n",
    "有时我们希望将某些计算移动到计算图之外，可以使用`Tensor.detach()`返回一个新的变量，该标量与原变量具有相同的值，但丢失计算图中如何计算原变量的信息。换句话说，梯度不会在该变量处继续向下传播。"
   ],
   "id": "518f1ff0962df102"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 分离张量",
   "id": "6dfdc0e38dadf260"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "有时候我们希望将某些计算移动到计算图之外，可以使用`Tensor.detach()`返回一个新的变量，该变量与原变量有相同的值，但丢失图中如何计算原变量的信息。换句话说，梯度不会在该变量出继续向下传播",
   "id": "8aafc15de1fc38c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.473139600Z",
     "start_time": "2025-12-22T04:36:31.377707800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x.detach()\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "# 不同的对象\n",
    "print(id(x))\n",
    "print(id(y))\n",
    "# 数据存放的位置是相同的\n",
    "print(x.untyped_storage().data_ptr())\n",
    "print(y.untyped_storage().data_ptr())"
   ],
   "id": "6602ce837ab1d605",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "1945804231680\n",
      "1947265010896\n",
      "5262610075648\n",
      "5262610075648\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.579522200Z",
     "start_time": "2025-12-22T04:36:31.475725200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x)\n",
    "y.add_(10)\n",
    "print(x)"
   ],
   "id": "956b02d66dcfb59c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(12., requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.642617700Z",
     "start_time": "2025-12-22T04:36:31.580524900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分别对x和y进行后续计算\n",
    "z1 = x ** 2\n",
    "z2 = y ** 2\n",
    "print(z1)\n",
    "print(z2)"
   ],
   "id": "7fa4d0c26b75cde1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(144., grad_fn=<PowBackward0>)\n",
      "tensor(144.)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.687589300Z",
     "start_time": "2025-12-22T04:36:31.644620200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将sum()作为损失函数进行反向传播\n",
    "z1.sum().backward()"
   ],
   "id": "b7b2bdbad87265e3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.738942700Z",
     "start_time": "2025-12-22T04:36:31.693597500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将sum()作为损失函数进行反向传播\n",
    "# z2.sum().backward() # 无法进行反向传播  element 0 of tensors does not require grad and does not have a grad_fn"
   ],
   "id": "484186cef2c07d1a",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.792574700Z",
     "start_time": "2025-12-22T04:36:31.738942700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x * x\n",
    "print(x)\n",
    "print(y)"
   ],
   "id": "1d06b95f912d7d1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.866635900Z",
     "start_time": "2025-12-22T04:36:31.794571100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分离y\n",
    "u = y.detach()\n",
    "print(u)"
   ],
   "id": "a533b3da12df3f02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.914380400Z",
     "start_time": "2025-12-22T04:36:31.867636100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # 定义z，让u参与新张量的计算\n",
    "# z1 = u * x\n",
    "# # 反向传播，计算梯度\n",
    "# z1.sum().backward()\n",
    "# # 查看x的梯度\n",
    "# print(x.grad)\n",
    "# print(x.grad == u)"
   ],
   "id": "67edb571b453e870",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:31.977110Z",
     "start_time": "2025-12-22T04:36:31.916386900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 让y参与新张量的计算\n",
    "z2 = y * x\n",
    "z2.sum().backward()\n",
    "print(x.grad)"
   ],
   "id": "56145209f0b813eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<hr/>\n",
    "\n",
    "`.data`和`.detach()`的对比"
   ],
   "id": "4439625552b1baa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.037135300Z",
     "start_time": "2025-12-22T04:36:31.978134700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 两组数据进行对比\n",
    "x1 = torch.tensor([1.0, 2, 3], requires_grad=True)\n",
    "x2 = torch.tensor([1.0, 2, 3], requires_grad=True)\n",
    "print(x1)\n",
    "print(x2)"
   ],
   "id": "6a093ffbadc52ffa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.098917200Z",
     "start_time": "2025-12-22T04:36:32.038135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y1 = x1.sigmoid()\n",
    "y2 = x2.sigmoid()\n",
    "print(y1)\n",
    "print(y2)"
   ],
   "id": "61eec3f910e5ea73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.164015800Z",
     "start_time": "2025-12-22T04:36:32.101924600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y1.sum().backward()\n",
    "# print(x1.grad)"
   ],
   "id": "54e14f5a3ab34719",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.215963500Z",
     "start_time": "2025-12-22T04:36:32.165009500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y2.sum().backward()\n",
    "# print(x2.grad)"
   ],
   "id": "ac369cfd68a0b8b9",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.289898Z",
     "start_time": "2025-12-22T04:36:32.219943400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z1 = y1.data\n",
    "z2 = y2.detach()\n",
    "print(z1)\n",
    "print(z2)\n",
    "print(z1.requires_grad)\n",
    "print(z2.requires_grad)"
   ],
   "id": "213159fa253c258e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.378107100Z",
     "start_time": "2025-12-22T04:36:32.315911200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z1.zero_()\n",
    "z2.zero_()\n",
    "print(y1)\n",
    "print(y2)"
   ],
   "id": "9efa9117cc32e71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0., 0., 0.], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:32.448950400Z",
     "start_time": "2025-12-22T04:36:32.381094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y1.sum().backward()\n",
    "print(x1.grad)  # 计算流程图没有改变，仅仅通过data修改了数值，导致梯度计算出现错误，不安全"
   ],
   "id": "294c02bc50a252b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T04:36:51.027510300Z",
     "start_time": "2025-12-22T04:36:50.637158300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y2.sum().backward()\n",
    "print(x2.grad)  # 报错，自动微分引擎发现数值修改，不允许梯度计算，安全"
   ],
   "id": "44160957dbf82bb6",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43my2\u001B[49m\u001B[43m.\u001B[49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(x2.grad)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
