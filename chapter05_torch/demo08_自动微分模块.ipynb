{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 自动微分模块",
   "id": "2f27f16c80634a1c"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "> 训练神经网络时，框架会根据设计好的模型构建一个计算图（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出，并通过反向传播算法来根据给定参数的损失函数的梯度调整参数（模型权重）。\n",
    "> PyTorch 具有一个内置的微分引擎`torch.autograd`以支持计算图的梯度自动计算。"
   ],
   "id": "4502d96e6b5268e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T04:00:30.670557800Z",
     "start_time": "2026-02-01T04:00:28.253116500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 输入x\n",
    "x = torch.tensor(10.)\n",
    "# 目标值y\n",
    "y = torch.tensor(3.).reshape(1, 1)\n",
    "\n",
    "# 初始化权重w\n",
    "w = torch.rand(1, 1, requires_grad=True)\n",
    "# 初始化偏置b\n",
    "b = torch.rand(1, 1, requires_grad=True)\n",
    "\n",
    "z = w * x + b\n",
    "\n",
    "# 设置损失函数\n",
    "loss = torch.nn.MSELoss()\n",
    "loss_value = loss(z, y)\n",
    "\n",
    "# 反向传播\n",
    "loss_value.backward()\n",
    "\n",
    "# 打印w，b的梯度\n",
    "w.grad, b.grad"
   ],
   "id": "6e2a057d4a06fd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[93.8544]]), tensor([[9.3854]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "该计算图中x、w、b 为叶子节点，即最基础的节点。叶子节点的数据并非由计算生成，因此是整个计算图的基石,叶子节点张量不可以执行`in-place`操作。而最终的loss为根节点。\n",
    "\n",
    "可通过`is_leaf`属性查看张量是否为叶子节点："
   ],
   "id": "b342adcc94038bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T04:00:30.750593100Z",
     "start_time": "2026-02-01T04:00:30.690567700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(x.is_leaf)\n",
    "print(w.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(z.is_leaf)\n",
    "print(y.is_leaf)\n",
    "print(loss_value.is_leaf)"
   ],
   "id": "c97976e694ad7393",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "自动微分的关键就是记录节点的数据与运算。数据记录在张量的`data`属性中，计算记录在张量的`grad_fn`属性中。\n",
    "\n",
    "计算图根据搭建方式可分为静态图和动态图，PyTorch是动态图机制，在计算的过程中逐步搭建计算图，同时对每个Tensor都存储`grad_fn`供自动微分使用。\n",
    "\n",
    "若设置张量参数`requires_grad=True`，则PyTorch会追踪所有基于该张量的操作，并在反向传播时计算其梯度。依赖于叶子节点的节点，`requires_grad`默认为True。当计算到根节点后，在根节点调用`backward()`方法即可反向传播计算计算图中所有节点的梯度。\n",
    "\n",
    "非叶子节点的梯度在反向传播之后会被释放掉（除非设置参数`retain_grad=True`）。而叶子节点的梯度在反向传播之后会保留（累积）。通常需要使用`optimizer.zero_grad()`清零参数的梯度。\n",
    "\n",
    "有时我们希望将某些计算移动到计算图之外，可以使用`Tensor.detach()`回一个新的变量，该变量与原变量具有相同的值，但丢失计算图中如何计算原变量的信息。换句话说，梯度不会在该变量处继续向下传播。"
   ],
   "id": "e81997efa9461d4b",
   "attachments": {
    "138e7ebe-53aa-46c7-b107-340be0b60eeb.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzIAAAInCAIAAADMO/vmAAAFNUlEQVR4Xu3BMQEAAADCoPVP7WcKoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgBulLQAB8iwOpwAAAABJRU5ErkJggg=="
    }
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T04:00:58.144911300Z",
     "start_time": "2026-02-01T04:00:58.051897600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.normal(mean=10, std=2, size=(2, 2), requires_grad=True)\n",
    "y = x * x\n",
    "u = y.detach()  # 分离y得到一个新变量\n",
    "\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "# 反向传播函数计算z=u*x关于x的偏导数时将u作为常数处理，而不是z=x*x*x关于x的偏导数\n",
    "x, y, z, u, x.grad == u"
   ],
   "id": "8ff647035d522a56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.1341, 10.7240],\n",
       "         [ 8.9739,  7.9776]], requires_grad=True),\n",
       " tensor([[102.6998, 115.0045],\n",
       "         [ 80.5315,  63.6413]], grad_fn=<MulBackward0>),\n",
       " tensor([[1040.7698, 1233.3101],\n",
       "         [ 722.6839,  507.7021]], grad_fn=<MulBackward0>),\n",
       " tensor([[102.6998, 115.0045],\n",
       "         [ 80.5315,  63.6413]]),\n",
       " tensor([[True, True],\n",
       "         [True, True]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
